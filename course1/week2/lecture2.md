# Week 2. Neural Networks Basics

## Logistic Regression as a Neural Network

### Binary Classification

  binary classification ë¬¸ì œì—ì„œ ê²°ê³¼ê°’ì€ 1 ë˜ëŠ” 0ì´ë‹¤.

#### Cat vs Non-Cat

  ì´ë¯¸ì§€ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê³ ì–‘ì´ ì´ë¯¸ì§€ì´ë©´ 1, ê³ ì–‘ì´ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë©´ 0ì„ ì˜ˆì¸¡í•˜ëŠ” ì˜ˆì œì´ë‹¤. ê³ ì–‘ì´ ì´ë¯¸ì§€ë¥¼ feature vector xë¡œ í‘œí˜„í•œ inputìœ¼ë¡œ classifierë¥¼ í•™ìŠµì‹œí‚¤ê³  label yë¥¼ ì˜ˆì¸¡í•´ë³¸ë‹¤. y = 1ì´ë©´ ê³ ì–‘ì´ ì´ë¯¸ì§€ì´ê³  y = 0ì´ë©´ ê³ ì–‘ì´ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë‹¤.

  ì´ë¯¸ì§€ëŠ” Red, Green, Blue ê°ê°ì˜ aìƒ‰ì— ëŒ€ì‘ë˜ëŠ” í–‰ë ¬ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. ì´ë¯¸ì§€ì˜ í•´ìƒë„ê°€ 64í”½ì…€ x 64í”½ì…€ì´ë¼ë©´ Red, Green, Blueê°€ ê°ê° 64 x 64 í–‰ë ¬ë¡œ í‘œí˜„ëœë‹¤. ì›ì†Œê°’ì€ í”½ì…€ì˜ intensity ê°’ì„ ë‚˜íƒ€ë‚¸ë‹¤.

  ![](img/1.png)

  ì´ë¯¸ì§€ë¥¼ feature vector xë¡œ í‘œí˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ì„¸ ê°€ì§€ ìƒ‰ì˜ ëª¨ë“  intensity ê°’ì„ n<sub>x</sub> x 1 ì°¨ì›ìœ¼ë¡œ ë³€í˜•ì‹œí‚¨ë‹¤. ì´ë•Œ n<sub>x</sub>ëŠ” ì„¸ ê°€ì§€ ìƒ‰ì˜ ëª¨ë“  í”½ì…€ê°’ë“¤ì˜ í¬ê¸°ì´ë¯€ë¡œ n<sub>x</sub> = 64 x 64 x 3(12288)ì´ë‹¤.

  ![](img/2.png)

### Logistic Regression

  Logistic regressionì€ supervised learning ë¬¸ì œì—ì„œ output ğ‘¦ê°€ 0 ë˜ëŠ” 1ì¼ ë•Œ ì‚¬ìš©í•˜ëŠ” learning algorithmì´ë‹¤. Logistic regressionì˜ ëª©ì ì€ ì˜ˆì¸¡ê°’ê³¼ íŠ¸ë ˆì´ë‹ ì‚¬ì´ì˜ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì´ë‹¤.

  $$x$$ ê°€ ì£¼ì–´ì¡Œì„ ë•Œ $$\hat{y} = P(y=1 | x)$$ ì´ë‹¤. $$\hat{y}$$ ì˜ ë²”ìœ„ëŠ” $$0 \leq \hat{y} \leq 1$$ ì´ë‹¤.

  * $$n_{x}$$ : feature ê°œìˆ˜
  * input feature vector : $$x \in \mathbb{R}^{n_{x}}$$
  * training label : $$y \in \{0, 1\}$$
  * Parameter : $$w \in \mathbb{R}^{n_{x}}, b \in \mathbb{R}$$
  * output : $$\hat{y} = \sigma(w^Tx + b) = \sigma(z) = \frac{1}{1 + e^{-z}}$$ : Sigmoid function

#### Sigmoid function

  ![](img/3.png)

  * Linear function $$z = w^Tx + b$$ì˜ ê²°ê³¼ê°’ì„ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ sigmoid functionì„ ì‚¬ìš©í•œë‹¤.
  * $$\lim_{z\to\infty} \sigma(z) = 1$$
  * $$\lim_{z\to-\infty} \sigma(z) = 0$$
  * $$z = 0 \to \sigma(z) = 0.5$$

### Logistic Regression: Cost Function

  $$\{(x^{(1)}, y^{(1)}), \dots , (x^{(m)},, y^{(m )} )\}$$ ì´ ì£¼ì–´ì¡Œì„ ë•Œ $$\hat{y}^{(i)} \approx y^{(i)}$$ ê°€ ë˜ì–´ì•¼ í•œë‹¤. ì°¸ê³ ë¡œ $$x^{(i)}$$ ëŠ” $$i$$ ë²ˆì§¸ training exampleì„ ì˜ë¯¸í•œë‹¤.

#### Loss(Error) function

  Loss functionì€ í•œ training exampleì— ëŒ€í•œ ì—ëŸ¬ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ë¡œ prediction $$\hat{y}$$ ì™€ output $$y$$ ì˜ ì°¨ì´ë¥¼ êµ¬í•œë‹¤.

  $$L(\hat{y}^{(i)}, y^{(i)}) =  -\{(y^{(i)}\log(\hat{y}^{(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(i)})\}$$

  * $$y^{(i)} = 1 \to L(\hat{y}^{(i)}, y^{(i)}) =  -\log(\hat{y}^{(i)})$$. ì—ëŸ¬ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ì„œëŠ”  $$-\log(\hat{y}^{(i)}) \to 0$$ ì´ë¯€ë¡œ $$\hat{y}^{(i)} \to 1$$ ì´ ëœë‹¤.
  * $$y^{(i)} = 0 \to L(\hat{y}^{(i)}, y^{(i)}) =  -\log(1 - \hat{y}^{(i)})$$. ì—ëŸ¬ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ì„œëŠ”  $$-\log(1- \hat{y}^{(i)}) \to 0$$ ì´ë¯€ë¡œ $$\hat{y}^{(i)} \to 0$$ ì´ ëœë‹¤.

#### Cost function

  Cost functionì€ ì „ì²´ training setì˜ loss functionì˜ í‰ê· ì´ë‹¤.

  $$J(w, b) = \frac{1}{m}\sum_{i = 1}^{m}L(\hat{y}^{(i)}, y^{(i)}) = -\frac{1}{m}\sum_{i = 1}^{m}\{y^{(i)}\log(\hat{y}^{(i)}) + (1 - y^{(i)})\log(1 - \hat{y}^{(i)})\}$$

### Gradient Descent

  Cost function $$J(w, b)$$ ë¥¼ ìµœì†Œí™”í•˜ëŠ” Parameter $$w$$, $$b$$ ë¥¼ êµ¬í•´ì•¼ í•œë‹¤.

  ![](img/4.png)
  repeat { <br>
  $$w := w - \alpha\frac{\partial J(w, b)}{\partial w}$$ <br>
  
  $$b := b - \alpha\frac{\partial J(w, b)}{\partial b}$$ <br>

  }
  * $$\alpha$$ : learning rate


## Python and Vectorization

### Vectorization

  $$w, x \in \mathbb{R}^{n_{x}}$$
  
#### Non-vectorized version

  ```py
  z = 0
  for i in range(n_x):
    z += w[i] * x[i]
  z += b
  ```
#### Vectorized version
  
  ```py
  z = np.dot(w.T, x) + b
  ```